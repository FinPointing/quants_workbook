# Exploratory data analysis

Welcome to Lab 3!

In our previous session we learnt about `R` packages, including how to install and load them. We talked about the main types of data used in social science research and how to represent them in `R`. Also, we played around with some datasets using some key functions, such as: `filter()`, `select()`, and `mutate()`. In this session we will focus on descriptive statistics. This includes the exploration and description of quantitative data.

## Importing and data wrangling

Today, we will be working with data generated by the [Access Research Knowledge (ARK)](https://www.ark.ac.uk/ARK/) hub. ARK conducts a series of surveys about society and life in Northern Ireland. At this time we will be working with the results of the [Northern Ireland Life and Times Survey (NILT)](https://www.ark.ac.uk/nilt/) for the year 2012. In particular, we will be using a teaching dataset that focuses on community relations and political attitudes. This includes background information of the participants and their household. Please take 5-10 minutes to read the documentation of this data set ([Click here to access the documentation](https://www.ark.ac.uk/teaching/NILT2012TeachingResources.pdf)). You will have to constantly consult this document to understand and use the data in the NILT. Thus, I recommend you to save the PDF file in your local drive.

### Downloading and reading the data

We will continue using R Studio Cloud, as we did in our previous labs. Please follow the next steps:

1. Go to your 'Quants lab group' in [RStudio Cloud](https://rstudio.cloud/) (if you have not joined a shared space, follow the instructions in Section 2.1 of [Lab 2](#lab2)).
2. Start the project called 'NILT' located in your lab group'.
3. Once you have initialized the project, generate a new `R` scrip file, and save it as 'Exploratory analysis'.
3. Load the `tidyverse` and `haven` packages. This last package is useful to import data from SPSS (the `tidyverse` package was pre-installed in your session). You can copy, paste, and run the following functions from your script:
```{r}
library(tidyverse)
library(haven)
```

Next, we will create a folder to store the data. Then, download and read the NILT data set, following the next steps:

1. From your script, create a new folder called 'data':
```{r eval=FALSE}
dir.create('data')
```
2. Download the data using the `download.file()` function. Remember that you have to specify the URL first, and the destination of the file second (including the folder).
```{r eval = FALSE}
download.file('https://www.ark.ac.uk/teaching/NILT2012GR.sav', 
              'data/nilt2012.sav')
```
3. Take a look to the 'Files' tab in pane 3, you will see a folder called 'data', click on it, and you will see the `nilt2012.sav` file.
```{r fig.cap="Cloud files.", echo=FALSE}
knitr::include_graphics("./images/rstudio_cloud_files.png")
```
4. To read this type of file use the `read_sav()` function. Read the `.sav` file and assign it to an object called `nilt`.
```{r}
nilt <- read_sav("data/nilt2012.sav")
```

<br>
And that's it! You should see a new data object in your 'Environment' tab (Pane 2) ready to be used. You can also see that this contains 1204 observations (rows) and 133 variables (columns). Lets glimpse our newly imported data and see the type of variables included.
```{r eval = FALSE}
glimpse(nilt)
```


### Data wrangling

As you can see from the result of glimpse, the class for practically all the variables is `<dbl+lbl>`. What does this mean? This happened because usually datasets use numbers to represent each of the categories/levels in categorical variables. These numbers are _labelled_ with their respective meaning. This is why we have a combination of value types (`<dbl+lbl>`). Take the example of the variable called `rsex`, as you can see from the values displayed using `glimpse()`, this includes numbers only, e.g. `1,1,2,2...`. This is because '1' represents 'Male' respondents and '2' represents 'Female' respondents. You can check this in the [documentation](https://www.ark.ac.uk/teaching/NILT2012TeachingResources.pdf) or running `print_labels(nilt$rsex)` in your console, which returns the numeric value and its respective label. As in `rsex`, this is the case for many other variables in this data set.

You should be aware that this type of 'mix' variable is a special case since we imported a file from a _foreign_ file that saves metadata for each variable (containing the names of the categories). As you learned in the last lab, in `R` we treat categorical variables as `factor`. Therefore, we will coerce some variables as `factor`. This time we will use the function `as_factor()` instead of the simple `factor()` that we used before. This is because `as_factor()` allows us to keep the names of each category in the variables. The syntax is exactly the same as before. Copy and run the following from your script:
```{r}
# Gender of the respondent
nilt <- nilt %>% mutate(rsex = as_factor(rsex))
# Highest Educational qualification
nilt <- nilt %>% mutate(highqual = as_factor(highqual))
# Religion
nilt <- nilt %>% mutate(religcat = as_factor(religcat))
# Politic identification
nilt <- nilt %>% mutate(uninatid = as_factor(uninatid))
# Happiness
nilt <- nilt %>% mutate(ruhappy = as_factor(ruhappy))
```

Notice from the code above that we are replacing the 'old' dataset with the result of the mutated variables that are of type `factor`. This is why we assigned the result with the _assigning operator_ `<-`. 

What about the numeric variables? In the documentation file there is a table in which you will see a type of measure 'scale'. This usually refers to continuous numeric variables (e.g. age or income).^[Be careful, in some cases these actually correspond to _discrete_ numeric values in this dataset (things that can be counted, e.g. number of...).] Let's coerce some variables to the appropriate type.

In the previous operation we coerced the variables as factor one by one, but we can transform several variables at once within the `mutate` function. As we did before, copy and run the following code in your script:
```{r}
# Coerce several variables as numeric
nilt <- nilt %>% 
  mutate(
    rage = as.numeric(rage),
    rhourswk = as.numeric(rhourswk),
    persinc2 = as.numeric(persinc2),
  )
```

Before doing some analyses, we will drop unused levels (or categories) in our dataset using the function `droplevels()`. Also, we will create a smaller subset that contains only the variables that we are interested in at the moment. To do this, use the `select()` function specifying the name of the original data first, and then the name of the variables we want to keep separated by a comma. Assign the result into a new object called `nilt_subset`.
```{r}
# drop unused levels
nilt <- droplevels(nilt)
#Subset
nilt_subset <- select(nilt, rsex, rage, highqual, religcat, uninatid,  ruhappy, rhourswk, persinc2)
```

Finally, save the NILT survey in an `.rds` file (this is the R format). We will not use this file now, but this will save us time formatting the data set in next labs (that is not having to repeat the steps above every time).
```{r}
saveRDS(nilt, "data/nilt_r_object.rds")
```

## Exploratory analysis
### Data overview

Are your summary statistics hiding something interesting?
```{r fig.cap="Exploratory analysis.", echo=FALSE}
knitr::include_graphics("./images/lab_3_summary_stats.png")
```

There are some basic functions that will allow you to understand and get familiar with the data. You used an important one already, `glimpse()`. But there are other useful ones. For instance, try `names()` to see a list of the variables' names, or have a look at the data using `View()`. 


To start exploring our data it will be useful to distinguish the adequate tools and measures available for the type of data we have. As you know now, there are two broad types: categorical and numeric.

There are several ways in which we can summarise our data. Today, we will use a useful package called `vtable`. Install it in your session running the following line in your console:
```{r eval = FALSE}
install.packages('vtable')
```

Once it is installed, make sure to load it with the next line. This time copy and paste it in your R script, so you can run it every time you restart your session.

```{r}
library(vtable)
```


### Categorical data

A usual way to explore the categorical data is using contingency and proportion tables. The contingency tables include the count for each category while the proportion tables contain the count divided by the total number of observations. 

Let's say we are interested in gender. We will use function `sumtable()` of the `vtable` package to produce a contingency table for a single variable (known as One-Way contingency table).
```{r}
sumtable(nilt_subset, vars = c('rsex'))
```

From the result, we see that there are more female respondents than males. 

Specifically, we see that males respondents represent 44.% of the total sample, whereas females 55.4%. 

We can do this with any type of categorical variable. Let's see how the sample is split by religion. So, we will add it to in the `vars` argument.

```{r}
sumtable(nilt_subset, vars = c('rsex', 'religcat'))
```

As you can see, about the same number of people are identified as being catholic or protestant, and a relatively small number with no religion.

What if we want to know the religious affiliation break-down by males and females. This is where Two-Way contingency tables are useful, and very common in quantitative research. To produce it, we have to specify the group argument in the `sumtable` function as following:
```{r}
sumtable(nilt_subset, vars = c('religcat'), group = 'rsex')
```

We can see some interesting results from this table. You can see that there are proportionally more female respondents who are either Catholic or Protestant than males, namely  43.5% vs 40.2% and 44.1% vs 40.6%, respectively. We also see that there are almost 20% of male respondents who do not self-identify with a religion which contrast to the 12% of female participants.


### Activity 1

From your R Studio Cloud script, do the following activities using the data stored in the `nilt_subset` object:

* Create a One-Way contingency table for `uninatid` using the `sumtable()` function;
* Using the variables `religcat` and `uninatid`, generate a Two-Way contingency table;
* Are your summary statistics hiding something interesting? Please discuss your results with your neighbour or your tutor.

### Continuous (numeric) data

In the previous section we learned how to summarise categorical data. But very often we want to work with continuous numeric variables or a combination of both. To summarise and understand numeric data there are two main types: measures of centrality and measures spread.

#### Measures of centrality

In quantitative research we usually have access to many observations in a sample which contains different attributes for each of them. It would be difficult (and probably not very useful) to talk about each of the NILT respondents one by one. Instead, to describe this sample we need measures that roughly represent all participants.

This is actually an important step in quantitative research, since it allows us to characterise the people that we are studying. For example, in the previous section we only talked about gender and political affiliation, but who are the people we are talking about? Probably a place to start digging deeper is to know their age. The first tool that we will use to understand numeric values is a histogram. Let's see how the age of NILT respondents is distributed.

```{r}
hist(nilt_subset$rage)
```

This plot show us on the X axis (horizontal) the age and the frequency on the Y axis. We can see that the youngest age in the sample is somewhere close to 20, and the oldest is almost 100. We also observe that the total number of observations (represented by the frequency on the vertical axis) for extreme values (close to 20 on the left-hand side and 100 on the right-hand side), tends to be lower that the values in the centre of the plot (somewhere between 30 and 45). For instance, we can see that there are approximately 120 respondents who are around 40 years old, that seems to be the most _popular_/frequent age in our sample. Now, we can represent these central values with actual measures, typically _mean_ or _median_.

The _median_ is the mid-point value in a numeric series. If you sort the values and split it by half, the value right in the middle is the median. Luckily there is a function ready to be used called... You guessed it - `median()`.
```{r}
median(nilt_subset$rage, na.rm = TRUE)
```
The median age is 48, that means that 50% (or half) of the respondents are equal or younger than this, and the other 50% is equal or older.^[Note that the argument `na.rm` equal `TRUE` is used in the function. The 'na' bit refers to missing values, and the 'rm' refers to remove. So we are telling R to remove the missing values when computing the median. This is because we do not know the age of 3 of the respondents in the sample.]

To compute the _mean_ manually, we need to sum all our values and divide it by the total number of the observations as follows: 
$$ mean =\frac{  x_1 + x_2 + x_3 ...+x_n } {n} $$
The formula above is for you to know that this measure considers the magnitude of all values included in the numeric series. Therefore, the average is sensitive to extreme numbers (e.g. a very very old person). To compute the mean you need the `mean()` function.
```{r}
mean(nilt_subset$rage, na.rm = T)
```

As you can see, the above measures try to approximate values that fall somewhere in the centre of the histogram plot, and represent all observations in the sample. They tell different things and are sometimes more (or less) suitable in a specific situation.

#### Measures of spread

By contrast, there are measures that helps us to describe how far away a series of numeric values are from the centre. The common measures of spread are _quartiles_, _variance_ and _standard deviation_.

The quartiles are very useful to quickly see how numeric data is distributed. Imagine that we sort all ages in the sample and split it in four equal parts. The first quartile includes the lowest 25% values, the second the other 25%, the third other other 25%, and the fourth the highest 25%. To compute quartiles we can use the `quantile` function.
```{r}
quantile(nilt_subset$rage, na.rm = T)
```
In our sample, the youngest quarter of the respondents is between 18 and 35 years old. The second quarter is between 35 and 48 years old. The next quartile is between 48 and 64. The oldest 25% of the respondents is between 64 and 97.

The _variance_ is useful to obtain a singe measure of spread (instead of four values, as the above) taking the mean as a reference. This is given by the following formula:

$$ var = \frac{ \sum(x - \bar{x})^2 }{n-1 } $$

To decipher the formula above, the $\bar{x}$ represents the mean, the $x$ represents each of the values in the numeric series. The formula takes each of the $x$ values and subtract it from the _mean_ $\bar{x}$. Later, it squares the the result of the subtraction (that is multiply it by itself). This is done to obtain a positive value, since some numbers in the series will be lower than the mean (resulting in negative values). Then, we sum all of them and divide the sum by the size/length of the numeric sequence $n$ minus 1. To estimate the variance in R we only need the `var()` function.

```{r}
var(nilt_subset$rage, na.rm = T)
```

As you can see, the result is not very intuitive. That is because we squared the subtraction. Luckily, there is a  measure that put it in _readable_ scale. This is the _standard deviation_. In essence this takes the square root of the variance (the reversed operation of squaring it):
$$sd=\sqrt{var}$$

To compute it in R use the `sd()` function.

```{r}
sd(nilt_subset$rage, na.rm = T)
```

This measure is more _human readable_ than the variance. Don't worry too much about needing to know how the formula works. Important thing to remember is what the measure represents. An informal definition of this measure is the average distance from the mean. It tell us how far the values in our data are from the mean.

#### Putting it all together

Phew, that was a lot!...

... Luckily we can use the `sumtable` function to compute all these measures at the same time! 

It is very simple. You can compute a quick summary for age as following:
```{r}
sumtable(nilt_subset, vars = c('rage'))
```

The result displays the number of observations used (N), the mean, the st. dev., minimum, the 1st (same as 'Pctl. 25') and 3rd quartile (same as 'Pctl. 75'), as well as the maximum (i.e., eldest respondent).

#### Categorical and numeric data

Lastly, there will be times in which you will need to compute a summary combining categorical and numeric data, to compare groups for example. The good news is that we can use exactly the same function to do this. Let's take the following example to compute the summary of age by gender:

```{r}
sumtable(nilt_subset, vars = c('rage'), group = 'rsex')
```

In the code above, we are simple specifying the variable age (`rage`) and grouping the summary by `rsex`. This produces a small summary included the number of observations in each category an the main measure of centrality and spread, namely the mean and the std. dev.

### Activity 2

Can money buy happiness?

Using the data in the `nilt_subset` object, complete the following activities. 

* Using the `hist()` function plot a histogram of personal income `persinc2`. From the NILT documentation this variable refers to annual personal income in £ before taxes and other deductions;
* Create a summary of the personal income `persinc2` variable, using the `sumtable()` function;
* Finally, compute the mean and standard deviation of the personal income `persinc2`, grouped by happiness `ruhappy`. What do you observe?... Even though we cannot answer the question above with this technique, it provides information about the association between personal income and happiness level. Spoiler-alert: The result is not very motivating. :(
* Discuss the results with your neighbour or your tutor.


